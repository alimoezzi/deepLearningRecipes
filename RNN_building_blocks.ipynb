{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# RNN Building Blocks: injecting recursivity to supervised learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class mySeries(Dataset):\n",
    "    \"\"\"Series for RNN dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,  length=1000, transform=None):\n",
    "        self.transform = transform\n",
    "        self.length = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        X = idx+2\n",
    "        y = idx+4\n",
    "\n",
    "        return X,y\n",
    "\n",
    "# mySeries dataset\n",
    "trainset = mySeries()\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "class RSuperviseL(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(1, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "\n",
    "    x= self.fc1(x)\n",
    "    return x\n",
    "\n",
    "model = RSuperviseL()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.001)\n",
    "num_epochs = 30\n",
    "train_tracker, test_tracker, accuracy_tracker = [], [], []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch(1/30) | Training loss: 0.02879007720912341 | \n",
      "Epoch(2/30) | Training loss: 0.001566351808833133 | \n",
      "Epoch(3/30) | Training loss: 0.0021744127825513715 | \n",
      "Epoch(4/30) | Training loss: 0.002156666609153035 | \n",
      "Epoch(5/30) | Training loss: 0.0021400008618002175 | \n",
      "Epoch(6/30) | Training loss: 0.002121673029790827 | \n",
      "Epoch(7/30) | Training loss: 0.0021082423869245304 | \n",
      "Epoch(8/30) | Training loss: 0.002092604770211892 | \n",
      "Epoch(9/30) | Training loss: 0.002076883069264568 | \n",
      "Epoch(10/30) | Training loss: 0.0020637031151693463 | \n",
      "Epoch(11/30) | Training loss: 0.0020496696981240348 | \n",
      "Epoch(12/30) | Training loss: 0.0020355489628798296 | \n",
      "Epoch(13/30) | Training loss: 0.002022556790876706 | \n",
      "Epoch(14/30) | Training loss: 0.0020103119763064115 | \n",
      "Epoch(15/30) | Training loss: 0.0019979172662658584 | \n",
      "Epoch(16/30) | Training loss: 0.0019865416383311185 | \n",
      "Epoch(17/30) | Training loss: 0.001974132897345271 | \n",
      "Epoch(18/30) | Training loss: 0.0019609402577884794 | \n",
      "Epoch(19/30) | Training loss: 0.0019507683442441248 | \n",
      "Epoch(20/30) | Training loss: 0.0019403377515061493 | \n",
      "Epoch(21/30) | Training loss: 0.0019294032115908522 | \n",
      "Epoch(22/30) | Training loss: 0.0019182294814470424 | \n",
      "Epoch(23/30) | Training loss: 0.0019079956187795233 | \n",
      "Epoch(24/30) | Training loss: 0.0018971824274558458 | \n",
      "Epoch(25/30) | Training loss: 0.0018873552233458213 | \n",
      "Epoch(26/30) | Training loss: 0.0018770025080812047 | \n",
      "Epoch(27/30) | Training loss: 0.0018670159637679262 | \n",
      "Epoch(28/30) | Training loss: 0.0018582267637725636 | \n",
      "Epoch(29/30) | Training loss: 0.0018487401974311979 | \n",
      "Epoch(30/30) | Training loss: 0.001839360673329793 | \n"
     ]
    }
   ],
   "source": [
    "for i in range(num_epochs):\n",
    "    cum_loss = 0\n",
    "\n",
    "    for batch, (X, y) in enumerate(trainloader,1):\n",
    "        X = X.type(torch.float32)\n",
    "        y = y.type(torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X.view(-1,1))\n",
    "        loss = criterion(output, y.view(-1,1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "    train_tracker.append(cum_loss/len(trainloader))\n",
    "    print(f\"Epoch({i+1}/{num_epochs}) | Training loss: {cum_loss/len(trainloader)} | \")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}