{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "from mypy.typeshed.stdlib.builtins import Exception\n",
    "from sphinx.testing.path import path\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no- raise -error\n",
    "!pip3 install jupyter==1.0.0\n",
    "!pip3 install numpy==1.23.1\n",
    "!pip3 install pandas==1.4.3\n",
    "!pip3 install pytorch-lightning==1.7.0\n",
    "!pip3 install scikit-learn==1.1.1\n",
    "!pip3 install scipy==1.8.1\n",
    "!pip3 install seaborn==0.11.2\n",
    "!pip3 install sklearn==0.0\n",
    "!pip3 install tensorboard==2.9.1\n",
    "!pip3 install tqdm==4.64.0\n",
    "!pip3 install torchinfo==1.7.0\n",
    "!pip3 install opencv-opencv-contrib-python==4.6.0.6\n",
    "!pip3 install scikit_image=0.19.3\n",
    "!pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F\n",
    "import pytorch_lightning as pl\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from torchinfo import summary\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    # Path directory for dataset\n",
    "    dataset_path=\"datasets/pokemon/sprites/pokemon\",\n",
    "    # Path fo model checkpoint\n",
    "    checkpoint_path=\".pytorch/pokemon_dcgan/\",\n",
    "    # Number of workers for dataloader\n",
    "    num_workers=8,\n",
    "    # Batch size during training\n",
    "    batch_size=128,\n",
    "    # Spatial size of training images. All images will be resized to this\n",
    "    #   size using a transformer.\n",
    "    image_size=64,\n",
    "    # Number of channels in the training images. For color images this is 3\n",
    "    num_channel=3,\n",
    "    # Size of z latent vector (i.e. size of generator input)\n",
    "    z_size=100,\n",
    "    # Size of feature maps in generator\n",
    "    generator_hidden_size=64,\n",
    "    # Generator network dimensions\n",
    "    gen_dims=(8, 4, 2, 1),\n",
    "    # Discriminator network dimensions\n",
    "    dis_dims=(1, 2, 4, 8),\n",
    "    # Size of feature maps in discriminator\n",
    "    discriminator_hidden_size=64,\n",
    "    # Number of training epochs\n",
    "    num_epochs=5,\n",
    "    # Learning rate for optimizers\n",
    "    lr=0.0002,\n",
    "    # Beta1 hyperparam for Adam optimizers\n",
    "    beta1=0.5,\n",
    "    beta2=0.999,\n",
    "    # Number of GPUs available. Use 0 for CPU mode.\n",
    "    ngpu=1,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and config['ngpu'] > 0) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%script false --no- raise -error\n",
    "!git clone https://github.com/PokeAPI/sprites.git datasets/pokemon/\n",
    "!mkdir -p {config['dataset_path']}/0\n",
    "!mv {config['dataset_path']}/*.png {config['dataset_path']}/0/0/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: CRC error\n"
     ]
    }
   ],
   "source": [
    "# New Dataset instance based on official artwork\n",
    "def transparent_white_bg_loader(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    if img.shape[-1] == 4:\n",
    "        alpha_channel = img[:, :, 3]\n",
    "        _, mask = cv2.threshold(alpha_channel, 254, 255, cv2.THRESH_BINARY)  # binarize mask\n",
    "        color = img[:, :, :3]\n",
    "        try:\n",
    "            color = cv2.bitwise_not(color, mask=mask)\n",
    "            new_img = cv2.bitwise_not(color)\n",
    "        except Exception as e:\n",
    "            raise Exception(f'{path}{color.shape}{mask.shape}')\n",
    "        return new_img\n",
    "    return img\n",
    "\n",
    "def is_valid(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    return (img is not None) and len(img.shape) == 3\n",
    "\n",
    "dataset = dset.ImageFolder(\n",
    "    root=Path(config['dataset_path']),\n",
    "    loader=transparent_white_bg_loader,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(config['image_size']),\n",
    "        transforms.CenterCrop(config['image_size']),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
    "        ),\n",
    "    ]),\n",
    "    is_valid_file=is_valid\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training images\")\n",
    "plt.imshow(\n",
    "    vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).permute([1, 2, 0]).cpu()\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Caught Exception in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/tmp/ipykernel_2691/4146227409.py\", line 5, in transparent_white_bg_loader\n    alpha_channel = img[:, :, 3]\nIndexError: index 3 is out of bounds for axis 2 with size 3\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sarm/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/sarm/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/sarm/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/sarm/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 230, in __getitem__\n    sample = self.loader(path)\n  File \"/tmp/ipykernel_2691/4146227409.py\", line 7, in transparent_white_bg_loader\n    raise Exception(img.shape)\nException: (48, 48, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mException\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [362]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m real_batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43miter\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m, \u001B[38;5;241m10\u001B[39m))\n\u001B[1;32m      3\u001B[0m plt\u001B[38;5;241m.\u001B[39maxis(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moff\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    528\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    529\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 530\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    531\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    532\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    533\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    534\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1224\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1222\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1223\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[0;32m-> 1224\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1250\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[0;34m(self, data)\u001B[0m\n\u001B[1;32m   1248\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[1;32m   1249\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[0;32m-> 1250\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1251\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[0;32m~/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/_utils.py:457\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[1;32m    454\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[1;32m    455\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[0;32m--> 457\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[0;31mException\u001B[0m: Caught Exception in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/tmp/ipykernel_2691/4146227409.py\", line 5, in transparent_white_bg_loader\n    alpha_channel = img[:, :, 3]\nIndexError: index 3 is out of bounds for axis 2 with size 3\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/sarm/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/sarm/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/sarm/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/sarm/.linuxbrew/opt/python@3.8/lib/python3.8/site-packages/torchvision/datasets/folder.py\", line 230, in __getitem__\n    sample = self.loader(path)\n  File \"/tmp/ipykernel_2691/4146227409.py\", line 7, in transparent_white_bg_loader\n    raise Exception(img.shape)\nException: (48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training images\")\n",
    "plt.imshow(\n",
    "    vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).permute([1, 2, 0]).cpu()\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv')!=-1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm')!=-1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_channel, gen_dims, negative_slope=0.2, kernel_size=4):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_channel = num_channel\n",
    "        self.input_size = input_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.features = self._make_layers(gen_dims, negative_slope)\n",
    "\n",
    "    def _make_layers(self, gen_dims, negative_slope):\n",
    "        layers = []\n",
    "        in_channels = self.input_size\n",
    "        for i, layer in enumerate(gen_dims):\n",
    "            layers += [\n",
    "                nn.ConvTranspose2d(\n",
    "                    in_channels,\n",
    "                    self.hidden_dim*layer,\n",
    "                    self.kernel_size,\n",
    "                    bias=False,\n",
    "                    stride=1 if i==0 else 2,\n",
    "                    padding=0 if i==0 else 1,\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.hidden_dim*layer),\n",
    "                nn.LeakyReLU(negative_slope=negative_slope),\n",
    "            ]\n",
    "            in_channels = self.hidden_dim*layer\n",
    "        layers += [\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                self.num_channel,\n",
    "                self.kernel_size,\n",
    "                bias=False,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.features(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the generator\n",
    "G = Generator(\n",
    "    config['z_size'],\n",
    "    config['generator_hidden_size'],\n",
    "    config['num_channel'],\n",
    "    config['gen_dims'],\n",
    ").to(device)\n",
    "\n",
    "# Random weights initialization\n",
    "#  to mean=0, stdev=0.02\n",
    "G.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "summary(G, (1, config['z_size'], 1, 1), col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Discriminator Network"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_channel, dis_dims, negative_slope=0.2, kernel_size=4):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_channel = num_channel\n",
    "        self.input_size = input_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.features = self._make_layers(dis_dims, negative_slope)\n",
    "\n",
    "    def _make_layers(self, dis_dims, negative_slope):\n",
    "        layers = []\n",
    "        in_channels = self.input_size\n",
    "        for i, layer in enumerate(dis_dims):\n",
    "            layers += [\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    self.hidden_dim*layer,\n",
    "                    self.kernel_size,\n",
    "                    bias=False,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                ),\n",
    "                nn.BatchNorm2d(self.hidden_dim*layer),\n",
    "                nn.LeakyReLU(negative_slope=negative_slope),\n",
    "            ]\n",
    "            in_channels = self.hidden_dim*layer\n",
    "        layers += [\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                self.num_channel,\n",
    "                self.kernel_size,\n",
    "                bias=False,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "            ),\n",
    "            nn.Sigmoid()\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.features(input)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Instantiate the discriminator\n",
    "D = Discriminator(\n",
    "    config['num_channel'],\n",
    "    config['discriminator_hidden_size'],\n",
    "    1,\n",
    "    config['dis_dims'],\n",
    ").to(device)\n",
    "\n",
    "# Random weights initialization\n",
    "#  to mean=0, stdev=0.02\n",
    "D.apply(weights_init)\n",
    "\n",
    "# Print the model\n",
    "print(D)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "summary(D, (1, config['num_channel'], config['image_size'], config['image_size']),\n",
    "        col_names=[\"input_size\", \"output_size\", \"kernel_size\", \"num_params\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DCGAN"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DCGAN(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            batch_size=128,\n",
    "            image_size=64,\n",
    "            num_channel=3,\n",
    "            z_size=100,\n",
    "            generator_hidden_size=64,\n",
    "            gen_dims=(8, 4, 2, 1),\n",
    "            dis_dims=(1, 2, 4, 8),\n",
    "            discriminator_hidden_size=64,\n",
    "            lr=0.0002,\n",
    "            beta1=0.5,\n",
    "            beta2=0.999,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.G = Generator(\n",
    "            z_size,\n",
    "            generator_hidden_size,\n",
    "            num_channel,\n",
    "            gen_dims,\n",
    "        )\n",
    "        G.apply(weights_init)\n",
    "        self.D = Discriminator(\n",
    "            num_channel,\n",
    "            discriminator_hidden_size,\n",
    "            1,\n",
    "            dis_dims,\n",
    "        )\n",
    "        D.apply(weights_init)\n",
    "\n",
    "        self.validation_z = torch.empty(\n",
    "            1,\n",
    "            z_size,\n",
    "            1,\n",
    "            1,\n",
    "            device=device\n",
    "        ).uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.G(z)\n",
    "\n",
    "    def loss_function(self, y_hat, y):\n",
    "        return F.binary_cross_entropy(y_hat, y)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        images, _ = batch\n",
    "\n",
    "        # input noise\n",
    "        z = torch.empty(\n",
    "            self.hparams.batch_size,\n",
    "            self.hparams.z_size,\n",
    "            1,\n",
    "            1,\n",
    "            device=device\n",
    "        ).uniform_(-1, 1)\n",
    "\n",
    "        # ground truth with label smoothing\n",
    "        labels = torch.full((self.hparams.batch_size,), 0.9, device=device)\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx==0:\n",
    "            self.generated_images = self(z)\n",
    "\n",
    "            # calculate loss for generator\n",
    "            g_loss = self.loss_function(self.D(self.generated_images).view(-1), labels)\n",
    "            self.log('g_loss', g_loss, prog_bar=True)\n",
    "            return g_loss\n",
    "\n",
    "        # train discriminator\n",
    "        elif optimizer_idx==1:\n",
    "            real_loss = self.loss_function(self.D(images).view(-1), labels)\n",
    "\n",
    "            labels = labels.fill_(0)\n",
    "            fake_loss = self.loss_function(self.D(self(z).detach()).view(-1), labels)\n",
    "\n",
    "            # discriminator loss is the sum loss of fake and real samples\n",
    "            d_loss =  fake_loss\n",
    "            self.log('d_loss', d_loss, prog_bar=True)\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        g_opt = torch.optim.Adam(\n",
    "            self.G.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            betas=(self.hparams.beta1, self.hparams.beta2)\n",
    "        )\n",
    "\n",
    "        d_opt = torch.optim.Adam(\n",
    "            self.D.parameters(),\n",
    "            lr=self.hparams.lr,\n",
    "            betas=(self.hparams.beta1, self.hparams.beta2)\n",
    "        )\n",
    "\n",
    "        return [g_opt, d_opt], []\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # log sampled images\n",
    "        sample_imgs = self(self.validation_z)\n",
    "        grid = vutils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checkpoint callback"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='checkpoint',\n",
    "    filename='pokemon_dcgan-{g_loss:.2f}-{d_loss:.2}',\n",
    "    save_top_k=1,\n",
    "    monitor='g_loss',\n",
    "    save_last=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Trainer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dccgan = DCGAN(\n",
    "    batch_size=config['batch_size'],\n",
    "    image_size=config['image_size'],\n",
    "    num_channel=config['num_channel'],\n",
    "    z_size=config['z_size'],\n",
    "    generator_hidden_size=config['generator_hidden_size'],\n",
    "    gen_dims=config['gen_dims'],\n",
    "    dis_dims=config['dis_dims'],\n",
    "    discriminator_hidden_size=config['discriminator_hidden_size'],\n",
    "    lr=config['lr'],\n",
    "    beta1=config['beta1'],\n",
    "    beta2=config['beta2'],\n",
    ")\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1 if torch.cuda.is_available() else None,\n",
    "    max_epochs=config['num_epochs'],\n",
    "    callbacks=[checkpoint_callback],\n",
    ")\n",
    "trainer.fit(dccgan, dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8ddef5fbe83c705d5a9598b88c7d6df5a96b3e1bfd6e35ff5406b9e3da66ceaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}