{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Semantic Segmentation using U-NET model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualization library\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "# data storing library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# torch libraries\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader, Dataset, sampler\n",
    "from torchvision import transforms\n",
    "# architecture and data split library\n",
    "from sklearn.model_selection import train_test_split\n",
    "# augmentation library\n",
    "from albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import os\n",
    "import pdb\n",
    "import time\n",
    "import warnings\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import concurrent.futures\n",
    "\n",
    "# warning print supression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fixing seed value"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def setSeed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "setSeed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# !mkdir -p '.pytorch/carvanasemseg/'\n",
    "# !mkdir -p '.pytorch/carvanasemseg/train_masks_png/'\n",
    "# !mkdir -p '.pytorch/carvanasemseg/train-128/'\n",
    "# !mkdir -p '.pytorch/carvanasemseg/train_masks-128/'\n",
    "\n",
    "def convert_img(fn):\n",
    "    PIL.Image.open(f'.pytorch/carvanasemseg/train_masks/{fn}').save(\n",
    "        f'.pytorch/carvanasemseg/train_masks_png/{fn[:-4]}.png')  #opening and saving image\n",
    "\n",
    "\n",
    "fns = os.listdir('.pytorch/carvanasemseg/train_masks')\n",
    "with concurrent.futures.ThreadPoolExecutor(8) as e:\n",
    "    e.map(convert_img, fns)\n",
    "\n",
    "\n",
    "def resize_mask(fn):\n",
    "    PIL.Image.open('.pytorch/carvanasemseg/train_masks_png/' + fn).resize((128, 128)).save(\n",
    "        f'.pytorch/carvanasemseg/train_masks-128/{fn}')\n",
    "\n",
    "\n",
    "fns = os.listdir('.pytorch/carvanasemseg/train_masks_png/')\n",
    "with concurrent.futures.ThreadPoolExecutor(8) as e:\n",
    "    e.map(resize_mask, fns)\n",
    "\n",
    "\n",
    "# we convert the high resolution input image to 128*128\n",
    "def resize_img(fn):\n",
    "    PIL.Image.open('.pytorch/carvanasemseg/train/' + fn).resize((128, 128)).save(\n",
    "        f'.pytorch/carvanasemseg/train-128/{fn}')\n",
    "\n",
    "\n",
    "fns = os.listdir('.pytorch/carvanasemseg/train/')\n",
    "with concurrent.futures.ThreadPoolExecutor(8) as e:\n",
    "    e.map(resize_img, fns)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "df = pd.read_csv('.pytorch/carvanasemseg/train_masks.csv')\n",
    "# location of original and mask image\n",
    "img_fol = '.pytorch/carvanasemseg/train-128/'\n",
    "mask_fol = '.pytorch/carvanasemseg/train_masks-128/'\n",
    "# imagenet mean/std will be used as the resnet backbone is trained on imagenet stats\n",
    "mean, std = (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                   img                                           rle_mask\n0  00087a6bd4dc_01.jpg  879386 40 881253 141 883140 205 885009 17 8850...\n1  00087a6bd4dc_02.jpg  873779 4 875695 7 877612 9 879528 12 881267 15...\n2  00087a6bd4dc_03.jpg  864300 9 866217 13 868134 15 870051 16 871969 ...\n3  00087a6bd4dc_04.jpg  879735 20 881650 26 883315 92 883564 30 885208...\n4  00087a6bd4dc_05.jpg  883365 74 883638 28 885262 119 885550 34 88716...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>img</th>\n      <th>rle_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00087a6bd4dc_01.jpg</td>\n      <td>879386 40 881253 141 883140 205 885009 17 8850...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00087a6bd4dc_02.jpg</td>\n      <td>873779 4 875695 7 877612 9 879528 12 881267 15...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00087a6bd4dc_03.jpg</td>\n      <td>864300 9 866217 13 868134 15 870051 16 871969 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00087a6bd4dc_04.jpg</td>\n      <td>879735 20 881650 26 883315 92 883564 30 885208...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00087a6bd4dc_05.jpg</td>\n      <td>883365 74 883638 28 885262 119 885550 34 88716...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transformation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# input-->\"phase\",mean,std\n",
    "# output-->list\n",
    "def get_transform(phase, mean, std):\n",
    "    list_trans = []\n",
    "    if phase == 'train':  # only flip during training\n",
    "        list_trans.extend([HorizontalFlip(p=0.5)])\n",
    "    list_trans.extend(\n",
    "        [Normalize(mean=mean, std=std, p=1), ToTensorV2()])  #normalizing the data & then converting to tensors\n",
    "    list_trans = Compose(list_trans)\n",
    "    return list_trans"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset and Dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "\n",
    "class CarDataset(Dataset):\n",
    "    def __init__(self, df, img_fol, mask_fol, mean, std, phase):\n",
    "        self.fname = df['img'].values.tolist()\n",
    "        self.img_fol = img_fol\n",
    "        self.mask_fol = mask_fol\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.phase = phase\n",
    "        self.trasnform = get_transform(phase, mean, std)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name = self.fname[idx]\n",
    "        img_name_path = os.path.join(self.img_fol, name)\n",
    "        mask_name_path = os.path.join(self.mask_fol, name.replace('.jpg', '_mask.png'))\n",
    "        img = cv2.imread(img_name_path)\n",
    "        mask = cv2.imread(mask_name_path, cv2.IMREAD_GRAYSCALE)\n",
    "        augmentation = self.trasnform(image=img, mask=mask)\n",
    "        img_aug = augmentation['image']  #[3,128,128] type:Tensor\n",
    "        mask_aug = augmentation['mask']  #[1,128,128] type:Tensor\n",
    "        return img_aug, mask_aug\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.fname)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def CarDataloader(df, img_fol, mask_fol, mean, std, batch_size, num_workers):\n",
    "    df_train, df_valid = train_test_split(df, test_size=0.2, random_state=69)\n",
    "    traindataset = CarDataset(df_train, img_fol, mask_fol, mean, std, 'train')\n",
    "    traindataloader = DataLoader(traindataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "    valdataset = CarDataset(df_valid, img_fol, mask_fol, mean, std, 'test')\n",
    "    valdataloader = DataLoader(valdataset, batch_size=batch_size, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return traindataloader, valdataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "NUM_WORKER = 0\n",
    "BATCH_SIZE = 1\n",
    "train_dl, val_dl = CarDataloader(df, img_fol, mask_fol, mean, std, batch_size=BATCH_SIZE, num_workers=NUM_WORKER)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 3, 128, 128]), torch.Size([1, 128, 128]))"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(iter(train_dl))\n",
    "xb.shape, yb.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## UNET Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "class UNET(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = self.contract_block(in_channels, 32, 7, 3)\n",
    "        self.conv2 = self.contract_block(32, 64, 3, 1)\n",
    "        self.conv3 = self.contract_block(64, 128, 3, 1)\n",
    "\n",
    "        self.upconv3 = self.expand_block(128, 64, 3, 1)\n",
    "        self.upconv2 = self.expand_block(64 * 2, 32, 3, 1)\n",
    "        self.upconv1 = self.expand_block(32 * 2, out_channels, 3, 1)\n",
    "\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "        contract = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        return contract\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
    "        expand = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        return expand\n",
    "\n",
    "    def forward(self, x):\n",
    "        # down-sampling\n",
    "        conv1 = self.conv1(x)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        conv3 = self.conv3(conv2)\n",
    "\n",
    "        # upsampling\n",
    "        upconv3 = self.upconv3(conv3)\n",
    "        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
    "        upconv1 = self.upconv1(torch.cat([upconv2, conv1], 1))\n",
    "\n",
    "        return upconv1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "unet = UNET(3, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "(torch.Size([1, 3, 128, 128]), torch.Size([1, 2, 128, 128]))"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = unet(xb)\n",
    "xb.shape, pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}