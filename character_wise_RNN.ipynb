{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "character_wise_RNN.ipynb",
   "provenance": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "sby3EvKFHscQ"
   },
   "source": [
    "# character-wise RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "DDzwBY87Hscd"
   },
   "source": [
    "![Overview](https://github.com/udacity/deep-learning/raw/78c91a5607ecfdc29b762e45c082d7ca5047c8a1/intro-to-rnns/assets/charseq.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "JG4ymNGZHsce"
   },
   "source": [
    "\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "u7IQJtsLHscg"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "P3SKxnVwHsch",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "outputId": "30227c70-2693-48d6-b4ab-8c3d4fb4f533"
   },
   "source": [
    "!curl http://www.gutenberg.org/files/2591/2591-0.txt --create-dirs -o .pytorch/trialReport/trial.txt\n",
    "with open('.pytorch/trialReport/trial.txt') as r:\n",
    "    reports = r.read()\n",
    "reports[:100]"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  547k    0   346    0     0    311      0  0:30:01  0:00:01  0:30:00   312\n",
      " 11  547k   11 64594    0     0  44486      0  0:00:12  0:00:01  0:00:11 44516\n",
      "100  547k  100  547k    0     0   269k      0  0:00:02  0:00:02 --:--:--  269k\n"
     ]
    },
    {
     "data": {
      "text/plain": "'ï»؟The Project Gutenberg EBook of Grimmsâ€™ Fairy Tales, by The Brothers Grimm\\n\\nThis eBook is for th'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r4evrtXuFkz-",
    "outputId": "287eec50-d2fa-4499-df82-369218808d92",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "source": [
    "import re\n",
    "words = re.split(r'\\W+', reports)\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ï', 'the', 'project', 'gutenberg', 'ebook', 'of', 'grimmsâ', 'fairy', 'tales', 'by', 'the', 'brothers', 'grimm', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'org', 'title', 'grimmsâ', 'fairy', 'tales', 'author', 'the', 'brothers', 'grimm', 'translator', 'edgar', 'taylor', 'and', 'marian', 'edwardes', 'posting', 'date', 'december', '14', '2008', 'ebook', '2591', 'release', 'date', 'april', '2001', 'last', 'updated', 'november', '7', '2016', 'language', 'english', 'character', 'set', 'encoding', 'utf', '8', 'start', 'of', 'this']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SBvMLLuvFko2"
   },
   "source": [
    "reports = ' '.join(words)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "rm5g3PVPHscj"
   },
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eK0cXQ7DHsck",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "cbcd55a9-fcf7-4f91-b414-6b37f77ef037"
   },
   "source": [
    "vocab = tuple(set(reports))\n",
    "print('vocabs:\\n', vocab)\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "print('int to vocab:\\n', int_to_vocab)\n",
    "vocab_to_int = {v: i for i,v in int_to_vocab.items()}\n",
    "encoded = np.array([vocab_to_int[v] for v in reports], dtype=np.int32)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabs:\n",
      " ('p', '1', 'r', 'i', 'ک', '0', 'ï', 'g', 'q', '7', 'w', 'u', 'n', 'a', '4', 'd', '5', '9', '3', 't', 'k', ' ', 'c', 'x', 'j', 's', 'y', 'œ', 'b', 'e', 'h', 'l', 'z', '_', '8', '2', 'm', 'â', '6', 'v', 'f', 'o')\n",
      "int to vocab:\n",
      " {0: 'p', 1: '1', 2: 'r', 3: 'i', 4: 'ک', 5: '0', 6: 'ï', 7: 'g', 8: 'q', 9: '7', 10: 'w', 11: 'u', 12: 'n', 13: 'a', 14: '4', 15: 'd', 16: '5', 17: '9', 18: '3', 19: 't', 20: 'k', 21: ' ', 22: 'c', 23: 'x', 24: 'j', 25: 's', 26: 'y', 27: 'œ', 28: 'b', 29: 'e', 30: 'h', 31: 'l', 32: 'z', 33: '_', 34: '8', 35: '2', 36: 'm', 37: 'â', 38: '6', 39: 'v', 40: 'f', 41: 'o'}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "iN-rQg0pHsck",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8c25972c-802d-4ede-d546-4a2312f02e64"
   },
   "source": [
    "encoded[:100]"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 6, 21, 19, 30, 29, 21,  0,  2, 41, 24, 29, 22, 19, 21,  7, 11, 19,\n       29, 12, 28, 29,  2,  7, 21, 29, 28, 41, 41, 20, 21, 41, 40, 21,  7,\n        2,  3, 36, 36, 25, 37, 21, 40, 13,  3,  2, 26, 21, 19, 13, 31, 29,\n       25, 21, 28, 26, 21, 19, 30, 29, 21, 28,  2, 41, 19, 30, 29,  2, 25,\n       21,  7,  2,  3, 36, 36, 21, 19, 30,  3, 25, 21, 29, 28, 41, 41, 20,\n       21,  3, 25, 21, 40, 41,  2, 21, 19, 30, 29, 21, 11, 25, 29])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "8eBm9UNSHscl"
   },
   "source": [
    "## One-hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "NNbhQvcMHscm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9e00af84-748e-4467-e46a-efbf6efc5a28"
   },
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "\n",
    "    return np.eye(n_labels,n_labels,  dtype=np.float32)[arr]\n",
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "_4ecvZkFHscm"
   },
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "QnwHn58kHscn"
   },
   "source": [
    "```\n",
    "                M steps ( seq length )\n",
    "               xxx                 xxx\n",
    "               x                     x\n",
    "               x                     x                                Starting sequence:\n",
    "               x                     x                                [1 2 3 4 5 6 7 8 9 10 11 12]\n",
    "               x                     x\n",
    "N batch size   x                     x                                Batch size = 2\n",
    "(No. of steps) x                     x                                [1 2 3 4 5 6]\n",
    "               x                     x                                [7 8 9 10 11 12]\n",
    "               x                     x\n",
    "               x                     x                                Seq length = 3\n",
    "               x                     x\n",
    "               x                     x                                  ┌─────┐\n",
    "               x                     x                                [ │1 2 3│ 4 5 6]\n",
    "               x                     x                                [ │7 8 9│ 10 11 12]\n",
    "               x                     x                                  └─────┘\n",
    "               xxx                 xxx\n",
    "\n",
    "            xxxxxxxxxxxxxx   xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "                         xxxxx\n",
    "                          xx\n",
    "\n",
    "                          k= No. of batches = total chars/ N.M\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Jq-s8InRHsco"
   },
   "source": [
    "def create_batches(arr, batch_size, seq_length):\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr) // batch_size_total\n",
    "    arr = arr[:n_batches*batch_size_total]\n",
    "    arr = arr.reshape((batch_size,-1))\n",
    "\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "KOuURObaHscp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fb4813d8-caa7-40b0-bd7a-159083843569"
   },
   "source": [
    "batches = create_batches(encoded, 8, 50)\n",
    "x, y= next(batches)\n",
    "print('x:\\n', x[:10, :10])\n",
    "print('\\ny:\\n', y[:10, :10])"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [[ 6 21 19 30 29 21  0  2 41 24]\n",
      " [11 19 21 19 30 29 21 29 31 15]\n",
      " [21 19 30 29 21 10 13 26 21 22]\n",
      " [21 15 41 21 26 41 11  2 21 19]\n",
      " [30 26 21 10 30 13 19 21 25 30]\n",
      " [21 37 21 30 13 12 25 21 10 29]\n",
      " [29  2 21  3 12 25 19 29 13 15]\n",
      " [21 28 11 19 21 25 30 11 15 15]]\n",
      "\n",
      "y:\n",
      " [[21 19 30 29 21  0  2 41 24 29]\n",
      " [19 21 19 30 29 21 29 31 15 29]\n",
      " [19 30 29 21 10 13 26 21 22 41]\n",
      " [15 41 21 26 41 11  2 21 19 13]\n",
      " [26 21 10 30 13 19 21 25 30 41]\n",
      " [37 21 30 13 12 25 21 10 29 12]\n",
      " [ 2 21  3 12 25 19 29 13 15 21]\n",
      " [28 11 19 21 25 30 11 15 15 29]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "2uvL80mYHscp"
   },
   "source": [
    "## Defining model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "05xGLCSRHscq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4c7acc76-425b-4ac7-b12a-613f899e8926"
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Pnq5Gc04Hscq"
   },
   "source": [
    "class textgenRNN(nn.Module):\n",
    "    def __init__(self,n_output, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.n_output = n_output\n",
    "\n",
    "        self.lstm = nn.LSTM(n_output, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "        self.fc1 = nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hid = self.lstm(x, hidden)\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "\n",
    "        out = self.fc1(out)\n",
    "        return out, hid\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.n_hidden).to(device),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.n_hidden).to(device))\n",
    "\n",
    "        return hidden"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "rLVfosNFHscr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "df4f72be-e43d-4f95-d641-ffcc31c49c87"
   },
   "source": [
    "from torchsummary import summary\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "model = textgenRNN(len(vocab), n_hidden=n_hidden, n_layers=n_layers)\n",
    "model.to(device)\n",
    "print(model)"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textgenRNN(\n",
      "  (lstm): LSTM(42, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=512, out_features=42, bias=True)\n",
      ")\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "J1EYiUkOHscs"
   },
   "source": [
    "import os\n",
    "if os.path.isdir('checkpoint') and os.path.isfile('./checkpoint/trialckpt.t7'):\n",
    "    model.load_state_dict(torch.load('./checkpoint/trialckpt.t7', map_location=torch.device(device)))"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "oL--pDxPHsct"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "t0HSH7QxHsct"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "epochs=100\n",
    "batch_size=32\n",
    "seq_length=64\n",
    "lr=0.01\n",
    "clip=5\n",
    "test_portion=0.1\n",
    "\n",
    "# change model mode to train\n",
    "model.train()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# create training and validation data\n",
    "data, test_data = train_test_split(encoded, test_size=test_portion, shuffle=False, random_state=0)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "i0ItBh6hHscv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "outputId": "a2a92903-f8ee-4685-dd50-c156489d506a"
   },
   "source": [
    "counter = 0\n",
    "n_vocab = len(vocab)\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = model.init_hidden(batch_size)\n",
    "\n",
    "    for x, y in create_batches(data, batch_size, seq_length):\n",
    "        counter += 1\n",
    "\n",
    "        # One-hot encode our data and make them Torch tensors\n",
    "        x = one_hot_encode(x, n_vocab)\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = model(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optim.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % 100 == 0:\n",
    "            # Get validation loss\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for x, y in create_batches(test_data, batch_size, seq_length):\n",
    "                # One-hot encode our data and make them Torch tensors\n",
    "                x = one_hot_encode(x, n_vocab)\n",
    "                x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                inputs, targets = x, y\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                output, val_h = model(inputs, val_h)\n",
    "                val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            print('==> Saving model ...')\n",
    "\n",
    "            if not os.path.isdir('checkpoint'):\n",
    "                os.mkdir('checkpoint')\n",
    "            torch.save(model.state_dict(), './checkpoint/trialckpt.t7')\n",
    "\n",
    "            model.train() # reset to train mode after iterationg through validation data\n",
    "\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving model ...\n",
      "Epoch: 1/100... Step: 100... Loss: 2.8470... Val Loss: 2.8846\n",
      "==> Saving model ...\n",
      "Epoch: 1/100... Step: 200... Loss: 2.5865... Val Loss: 2.6741\n",
      "==> Saving model ...\n",
      "Epoch: 2/100... Step: 300... Loss: 2.4895... Val Loss: 2.5240\n",
      "==> Saving model ...\n",
      "Epoch: 2/100... Step: 400... Loss: 2.3406... Val Loss: 2.4048\n",
      "==> Saving model ...\n",
      "Epoch: 3/100... Step: 500... Loss: 2.2576... Val Loss: 2.3761\n",
      "==> Saving model ...\n",
      "Epoch: 3/100... Step: 600... Loss: 2.2109... Val Loss: 2.2806\n",
      "==> Saving model ...\n",
      "Epoch: 4/100... Step: 700... Loss: 2.1588... Val Loss: 2.2261\n",
      "==> Saving model ...\n",
      "Epoch: 4/100... Step: 800... Loss: 2.0656... Val Loss: 2.1785\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "OJWkbwyKHsc5"
   },
   "source": [
    "# Predicting next character"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "0a1qYFEEHsc-"
   },
   "source": [
    "def predict(model, inputs, h=None, top_k= None):\n",
    "    x = np.array([[vocab_to_int[inputs]]])\n",
    "    x = one_hot_encode(x, len(vocab))\n",
    "    inp = torch.from_numpy(x).to(device)\n",
    "\n",
    "    # get access to hidden state\n",
    "    h = tuple([i.data for i in h])\n",
    "    # pass inputs and hidden state to model\n",
    "    out, h = model(inp, h)\n",
    "\n",
    "    # get prob of the char\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    p = p.to('cpu')\n",
    "\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(vocab))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "    p = p.numpy().squeeze()\n",
    "    v = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "    return int_to_vocab[v], h\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "O_gxWBx9Hsc_"
   },
   "source": [
    "# Sample text"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    },
    "id": "exOuMPOxHsc_"
   },
   "source": [
    "def sample(model, size, prime='the ', top_k=None):\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval() # eval mode\n",
    "\n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(model, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(model, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n",
    "print(sample(model, 2000, prime='the ', top_k=26))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}