{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "character_wise_RNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "sby3EvKFHscQ"
      },
      "source": [
        "# character-wise RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DDzwBY87Hscd"
      },
      "source": [
        "![Overview](https://github.com/udacity/deep-learning/raw/78c91a5607ecfdc29b762e45c082d7ca5047c8a1/intro-to-rnns/assets/charseq.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JG4ymNGZHsce"
      },
      "source": [
        "\n",
        "import time\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u7IQJtsLHscg"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P3SKxnVwHsch",
        "outputId": "52104625-8516-466f-91ab-befab2063670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "!curl https://raw.githubusercontent.com/udacity/deep-learning/master/intro-to-rnns/anna.txt > .pytorch/trialReport/trial.txt\n",
        "with open('.pytorch/trialReport/trial.txt') as r:\n",
        "    reports = r.read()\n",
        "reports[:100]"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1978k  100 1978k    0     0  14.2M      0 --:--:-- --:--:-- --:--:-- 14.2M\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rm5g3PVPHscj"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eK0cXQ7DHsck",
        "outputId": "5bceb6ea-7659-4b88-e6ee-0436fa3909e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab = sorted(set(reports))\n",
        "print('vocabs:\\n', vocab)\n",
        "int_to_vocab = dict(enumerate(vocab))\n",
        "print('int to vocab:\\n', int_to_vocab)\n",
        "vocab_to_int = {v: i for i,v in int_to_vocab.items()}\n",
        "encoded = np.array([vocab_to_int[v] for v in reports], dtype=np.int32)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabs:\n",
            " ['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "int to vocab:\n",
            " {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: '$', 5: '%', 6: '&', 7: \"'\", 8: '(', 9: ')', 10: '*', 11: ',', 12: '-', 13: '.', 14: '/', 15: '0', 16: '1', 17: '2', 18: '3', 19: '4', 20: '5', 21: '6', 22: '7', 23: '8', 24: '9', 25: ':', 26: ';', 27: '?', 28: '@', 29: 'A', 30: 'B', 31: 'C', 32: 'D', 33: 'E', 34: 'F', 35: 'G', 36: 'H', 37: 'I', 38: 'J', 39: 'K', 40: 'L', 41: 'M', 42: 'N', 43: 'O', 44: 'P', 45: 'Q', 46: 'R', 47: 'S', 48: 'T', 49: 'U', 50: 'V', 51: 'W', 52: 'X', 53: 'Y', 54: 'Z', 55: '_', 56: '`', 57: 'a', 58: 'b', 59: 'c', 60: 'd', 61: 'e', 62: 'f', 63: 'g', 64: 'h', 65: 'i', 66: 'j', 67: 'k', 68: 'l', 69: 'm', 70: 'n', 71: 'o', 72: 'p', 73: 'q', 74: 'r', 75: 's', 76: 't', 77: 'u', 78: 'v', 79: 'w', 80: 'x', 81: 'y', 82: 'z'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iN-rQg0pHsck",
        "outputId": "7aac65c7-b6b7-4404-a9af-8c66b8f1e9ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([31, 64, 57, 72, 76, 61, 74,  1, 16,  0,  0,  0, 36, 57, 72, 72, 81,\n",
              "        1, 62, 57, 69, 65, 68, 65, 61, 75,  1, 57, 74, 61,  1, 57, 68, 68,\n",
              "        1, 57, 68, 65, 67, 61, 26,  1, 61, 78, 61, 74, 81,  1, 77, 70, 64,\n",
              "       57, 72, 72, 81,  1, 62, 57, 69, 65, 68, 81,  1, 65, 75,  1, 77, 70,\n",
              "       64, 57, 72, 72, 81,  1, 65, 70,  1, 65, 76, 75,  1, 71, 79, 70,  0,\n",
              "       79, 57, 81, 13,  0,  0, 33, 78, 61, 74, 81, 76, 64, 65, 70],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8eBm9UNSHscl"
      },
      "source": [
        "## One-hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NNbhQvcMHscm",
        "outputId": "ed765417-6731-4fd0-ef1e-27fe024c8763",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    return np.eye(n_labels,n_labels,  dtype=np.float32)[arr]\n",
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_4ecvZkFHscm"
      },
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QnwHn58kHscn"
      },
      "source": [
        "                M steps ( seq length )\n",
        "               xxx                 xxx\n",
        "               x                     x\n",
        "               x                     x                                Starting sequence:\n",
        "               x                     x                                [1 2 3 4 5 6 7 8 9 10 11 12]\n",
        "               x                     x\n",
        "N batch size   x                     x                                Batch size = 2\n",
        "(No. of steps) x                     x                                [1 2 3 4 5 6]\n",
        "               x                     x                                [7 8 9 10 11 12]\n",
        "               x                     x\n",
        "               x                     x                                Seq length = 3\n",
        "               x                     x\n",
        "               x                     x                                  ┌─────┐\n",
        "               x                     x                                [ │1 2 3│ 4 5 6]\n",
        "               x                     x                                [ │7 8 9│ 10 11 12]\n",
        "               x                     x                                  └─────┘\n",
        "               xxx                 xxx\n",
        "\n",
        "            xxxxxxxxxxxxxx   xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
        "                         xxxxx\n",
        "                          xx\n",
        "\n",
        "                          k= No. of batches = total chars/ N.M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Jq-s8InRHsco"
      },
      "source": [
        "def create_batches(arr, batch_size, seq_length):\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    arr = arr[:n_batches*batch_size_total]\n",
        "    arr = arr.reshape((batch_size,-1))\n",
        "\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KOuURObaHscp",
        "outputId": "fef9bec7-5ed4-4de6-926c-80c177134a4b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batches = create_batches(encoded, 8, 50)\n",
        "x, y= next(batches)\n",
        "print('x:\\n', x[:10, :10])\n",
        "print('\\ny:\\n', y[:10, :10])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            " [[31 64 57 72 76 61 74  1 16  0]\n",
            " [75 71 70  1 76 64 57 76  1 57]\n",
            " [61 70 60  1 71 74  1 57  1 62]\n",
            " [75  1 76 64 61  1 59 64 65 61]\n",
            " [ 1 75 57 79  1 64 61 74  1 76]\n",
            " [59 77 75 75 65 71 70  1 57 70]\n",
            " [ 1 29 70 70 57  1 64 57 60  1]\n",
            " [43 58 68 71 70 75 67 81 13  1]]\n",
            "\n",
            "y:\n",
            " [[64 57 72 76 61 74  1 16  0  0]\n",
            " [71 70  1 76 64 57 76  1 57 76]\n",
            " [70 60  1 71 74  1 57  1 62 71]\n",
            " [ 1 76 64 61  1 59 64 65 61 62]\n",
            " [75 57 79  1 64 61 74  1 76 61]\n",
            " [77 75 75 65 71 70  1 57 70 60]\n",
            " [29 70 70 57  1 64 57 60  1 75]\n",
            " [58 68 71 70 75 67 81 13  1  3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "2uvL80mYHscp"
      },
      "source": [
        "## Defining model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "05xGLCSRHscq",
        "outputId": "3e4ae465-4740-4607-ff53-899c3f060f97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Pnq5Gc04Hscq"
      },
      "source": [
        "class textgenRNN(nn.Module):\n",
        "    def __init__(self,n_output, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.lstm = nn.LSTM(n_output, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(drop_prob)\n",
        "        self.fc1 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        out, hid = self.lstm(x, hidden)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        return out, hid\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rLVfosNFHscr",
        "outputId": "0d1f5d19-fa98-4957-89bd-a5db78e7954d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "n_hidden=512\n",
        "n_layers=3\n",
        "model = textgenRNN(len(vocab), n_hidden=n_hidden, n_layers=n_layers)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "textgenRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J1EYiUkOHscs"
      },
      "source": [
        "import os\n",
        "if os.path.isdir('checkpoint') and os.path.isfile('./checkpoint/trialckpt.t7'):\n",
        "    model.load_state_dict(torch.load('./checkpoint/trialckpt.t7', map_location=torch.device(device)))"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oL--pDxPHsct"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "t0HSH7QxHsct",
        "outputId": "4bd4ed96-3a86-4009-a02a-109de8fc3f38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "epochs=100\n",
        "batch_size=125\n",
        "seq_length=100\n",
        "lr=0.05\n",
        "clip=5\n",
        "test_portion=0.2\n",
        "\n",
        "# change model mode to train\n",
        "model.train()\n",
        "\n",
        "optim = torch.optim.Adagrad(model.parameters(), lr=lr, lr_decay=0.0001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# create training and validation data\n",
        "data, test_data = train_test_split(encoded, test_size=test_portion)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "textgenRNN(\n",
              "  (lstm): LSTM(83, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout1): Dropout(p=0.5, inplace=False)\n",
              "  (fc1): Linear(in_features=512, out_features=83, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "i0ItBh6hHscv",
        "outputId": "71ac75e8-3243-4fbe-f090-127fb82b87b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "counter = 0\n",
        "n_vocab = len(vocab)\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "\n",
        "    for x, y in create_batches(data, batch_size, seq_length):\n",
        "        counter += 1\n",
        "\n",
        "        # One-hot encode our data and make them Torch tensors\n",
        "        x = one_hot_encode(x, n_vocab)\n",
        "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = model(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optim.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % 100 == 0:\n",
        "            # Get validation loss\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for x, y in create_batches(test_data, batch_size, seq_length):\n",
        "                # One-hot encode our data and make them Torch tensors\n",
        "                x = one_hot_encode(x, n_vocab)\n",
        "                x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                inputs, targets = x, y\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                output, val_h = model(inputs, val_h)\n",
        "                val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            print('==> Saving model ...')\n",
        "\n",
        "            if not os.path.isdir('checkpoint'):\n",
        "                os.mkdir('checkpoint')\n",
        "            torch.save(model.state_dict(), './checkpoint/trialckpt.t7')\n",
        "\n",
        "            model.train() # reset to train mode after iterationg through validation data\n",
        "\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Saving model ...\n",
            "Epoch: 1/100... Step: 100... Loss: 3.1150... Val Loss: 3.1057\n",
            "==> Saving model ...\n",
            "Epoch: 2/100... Step: 200... Loss: 3.1270... Val Loss: 3.1066\n",
            "==> Saving model ...\n",
            "Epoch: 3/100... Step: 300... Loss: 3.1287... Val Loss: 3.1058\n",
            "==> Saving model ...\n",
            "Epoch: 4/100... Step: 400... Loss: 3.1007... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 4/100... Step: 500... Loss: 3.0965... Val Loss: 3.1052\n",
            "==> Saving model ...\n",
            "Epoch: 5/100... Step: 600... Loss: 3.1261... Val Loss: 3.1053\n",
            "==> Saving model ...\n",
            "Epoch: 6/100... Step: 700... Loss: 3.1104... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 7/100... Step: 800... Loss: 3.1101... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 8/100... Step: 900... Loss: 3.1076... Val Loss: 3.1052\n",
            "==> Saving model ...\n",
            "Epoch: 8/100... Step: 1000... Loss: 3.1009... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 9/100... Step: 1100... Loss: 3.0918... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 10/100... Step: 1200... Loss: 3.0976... Val Loss: 3.1052\n",
            "==> Saving model ...\n",
            "Epoch: 11/100... Step: 1300... Loss: 3.0964... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 12/100... Step: 1400... Loss: 3.1236... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 12/100... Step: 1500... Loss: 3.1050... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 13/100... Step: 1600... Loss: 3.0906... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 14/100... Step: 1700... Loss: 3.1118... Val Loss: 3.1052\n",
            "==> Saving model ...\n",
            "Epoch: 15/100... Step: 1800... Loss: 3.0999... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 15/100... Step: 1900... Loss: 3.1010... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 16/100... Step: 2000... Loss: 3.1087... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 17/100... Step: 2100... Loss: 3.0974... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 18/100... Step: 2200... Loss: 3.1033... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 19/100... Step: 2300... Loss: 3.1028... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 19/100... Step: 2400... Loss: 3.1016... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 20/100... Step: 2500... Loss: 3.1083... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 21/100... Step: 2600... Loss: 3.1123... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 22/100... Step: 2700... Loss: 3.1030... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 23/100... Step: 2800... Loss: 3.1128... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 23/100... Step: 2900... Loss: 3.0976... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 24/100... Step: 3000... Loss: 3.1097... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 25/100... Step: 3100... Loss: 3.0871... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 26/100... Step: 3200... Loss: 3.0880... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 26/100... Step: 3300... Loss: 3.1154... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 27/100... Step: 3400... Loss: 3.1133... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 28/100... Step: 3500... Loss: 3.1052... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 29/100... Step: 3600... Loss: 3.1007... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 30/100... Step: 3700... Loss: 3.1036... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 30/100... Step: 3800... Loss: 3.1011... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 31/100... Step: 3900... Loss: 3.1240... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 32/100... Step: 4000... Loss: 3.1033... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 33/100... Step: 4100... Loss: 3.1074... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 34/100... Step: 4200... Loss: 3.1116... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 34/100... Step: 4300... Loss: 3.0973... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 35/100... Step: 4400... Loss: 3.1140... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 36/100... Step: 4500... Loss: 3.1098... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 37/100... Step: 4600... Loss: 3.0983... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 38/100... Step: 4700... Loss: 3.1129... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 38/100... Step: 4800... Loss: 3.1157... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 39/100... Step: 4900... Loss: 3.1153... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 40/100... Step: 5000... Loss: 3.0893... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 41/100... Step: 5100... Loss: 3.1192... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 41/100... Step: 5200... Loss: 3.1129... Val Loss: 3.1050\n",
            "==> Saving model ...\n",
            "Epoch: 42/100... Step: 5300... Loss: 3.0927... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 43/100... Step: 5400... Loss: 3.1063... Val Loss: 3.1051\n",
            "==> Saving model ...\n",
            "Epoch: 44/100... Step: 5500... Loss: 3.1235... Val Loss: 3.1050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-b626db5a0190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# calculate the loss and perform backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OJWkbwyKHsc5"
      },
      "source": [
        "# Predicting next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0a1qYFEEHsc-"
      },
      "source": [
        "def predict(model, inputs, h=None, top_k= None):\n",
        "    x = np.array([[vocab_to_int[inputs]]])\n",
        "    x = one_hot_encode(x, len(vocab))\n",
        "    inp = torch.from_numpy(x).to(device)\n",
        "\n",
        "    # get access to hidden state\n",
        "    h = tuple([i.data for i in h])\n",
        "    # pass inputs and hidden state to model\n",
        "    out, h = model(inp, h)\n",
        "\n",
        "    # get prob of the char\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    p = p.to('cpu')\n",
        "\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(vocab))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    p = p.numpy().squeeze()\n",
        "    v = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "    return int_to_vocab[v], h\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "O_gxWBx9Hsc_"
      },
      "source": [
        "# Sample text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "exOuMPOxHsc_",
        "outputId": "8b636408-8af6-467a-83c1-7cda22d430dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def sample(model, size, prime='The ', top_k=None):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval() # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = model.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)\n",
        "print(sample(model, 1000, prime='The ', top_k=26))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The rnfyd,leshs, ,t,oiay  esklt erleot,welttoiftiekeehatmirik,aapg  t whnouhmegonecmmhebgernefytarndais ho eeriahmn atecs euhfoetpleg.n asertweih\n",
            "\n",
            "t li   ee tihontmlgserl ai ymr mrtloiopdrm lo r aanfg  oast \n",
            "nmyeeatant s t toav  \n",
            "edr itarrnvvpehowai c ny edy\n",
            "rieiy lyh\n",
            " ,eir npdastdstke\n",
            "aoa sult\n",
            "  ntgto eewts w r oaenlh.o  ho eha .iiiv  hft  iaht nw\n",
            " uinndeoc.bayecetpaa teriwofeeofyhit suvgnelfrprhhakh .td  uutyyott oe losn y a teaeeth oigkuhcsstvhni  hesanus h     fftn nguhg,mlyyliru,t  tu a  e oeelamt heel\n",
            "idaismnta nip\n",
            " s,ri \n",
            "face woeikoieteste ei\n",
            "to upac yk \n",
            "lonhnmin\n",
            "hh i,ttnshtmi i la h\n",
            "dchna ik ua ogas slr hed ecrenoewohe tceeonatiethditae s a itw  s iynew irm\n",
            "wnsss    sofstrcnegwso enth eaansae nto.a niret,s skhhtfuatnut e   tlhdo ry f,hrcom,myssa   o k wttterili,ts dtthev toel tyosf l ua \n",
            "drmwa ea.doh mcrld ewadsntlracetd c h glodhwtirp\n",
            "w h st\n",
            "\n",
            "sil yhfnanh uy lao nrat fsatie\n",
            "rn\n",
            "se d paic\n",
            "helnwere r,tpn\n",
            "gdoutdtta,cyo ,inniatrgeitkhmh e  dawdn  nnlt, nn o.one o  aa psovecelrdeelw \n",
            "gp u\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}