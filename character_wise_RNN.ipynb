{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "character_wise_RNN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "sby3EvKFHscQ"
      },
      "source": [
        "# character-wise RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DDzwBY87Hscd"
      },
      "source": [
        "![Overview](https://github.com/udacity/deep-learning/raw/78c91a5607ecfdc29b762e45c082d7ca5047c8a1/intro-to-rnns/assets/charseq.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JG4ymNGZHsce"
      },
      "source": [
        "\n",
        "import time\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u7IQJtsLHscg"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P3SKxnVwHsch",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f31c1e61-2196-49a9-81e4-4e1ccfdb311a"
      },
      "source": [
        "!curl https://raw.githubusercontent.com/udacity/deep-learning/master/intro-to-rnns/anna.txt --create-dirs -o .pytorch/trialReport/trial.txt\n",
        "with open('.pytorch/trialReport/trial.txt') as r:\n",
        "    reports = r.read()\n",
        "reports[:100]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 1978k  100 1978k    0     0  7696k      0 --:--:-- --:--:-- --:--:-- 7696k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rm5g3PVPHscj"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eK0cXQ7DHsck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b5c2f00-5c8a-43ba-f89a-ab79a55d2f80"
      },
      "source": [
        "vocab = tuple(set(reports))\n",
        "print('vocabs:\\n', vocab)\n",
        "int_to_vocab = dict(enumerate(vocab))\n",
        "print('int to vocab:\\n', int_to_vocab)\n",
        "vocab_to_int = {v: i for i,v in int_to_vocab.items()}\n",
        "encoded = np.array([vocab_to_int[v] for v in reports], dtype=np.int32)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabs:\n",
            " ('X', 'y', 'D', 'p', '?', '4', 'z', 'R', 'I', 'i', 'v', '9', 'w', 'T', 'F', '8', 'J', 'Y', 'K', 'S', 'W', '5', '1', 'M', 'U', 'a', 'k', ':', '3', 'H', 'g', 'o', 'u', 'm', '\\n', '!', ' ', '(', ')', 'r', 'd', 'C', 'e', 'N', 'E', 'c', 'L', 'P', 'b', '_', '*', 'A', 's', '&', '-', '0', 'V', '2', 'l', 'O', '.', '`', '7', 'f', '/', \"'\", 'j', 'x', 'G', '%', 'Q', ',', ';', 'Z', 'n', 'q', 'h', 't', '\"', '@', '6', 'B', '$')\n",
            "int to vocab:\n",
            " {0: 'X', 1: 'y', 2: 'D', 3: 'p', 4: '?', 5: '4', 6: 'z', 7: 'R', 8: 'I', 9: 'i', 10: 'v', 11: '9', 12: 'w', 13: 'T', 14: 'F', 15: '8', 16: 'J', 17: 'Y', 18: 'K', 19: 'S', 20: 'W', 21: '5', 22: '1', 23: 'M', 24: 'U', 25: 'a', 26: 'k', 27: ':', 28: '3', 29: 'H', 30: 'g', 31: 'o', 32: 'u', 33: 'm', 34: '\\n', 35: '!', 36: ' ', 37: '(', 38: ')', 39: 'r', 40: 'd', 41: 'C', 42: 'e', 43: 'N', 44: 'E', 45: 'c', 46: 'L', 47: 'P', 48: 'b', 49: '_', 50: '*', 51: 'A', 52: 's', 53: '&', 54: '-', 55: '0', 56: 'V', 57: '2', 58: 'l', 59: 'O', 60: '.', 61: '`', 62: '7', 63: 'f', 64: '/', 65: \"'\", 66: 'j', 67: 'x', 68: 'G', 69: '%', 70: 'Q', 71: ',', 72: ';', 73: 'Z', 74: 'n', 75: 'q', 76: 'h', 77: 't', 78: '\"', 79: '@', 80: '6', 81: 'B', 82: '$'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iN-rQg0pHsck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3af38bf0-f102-4100-a351-dea1a781f2be"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([41, 76, 25,  3, 77, 42, 39, 36, 22, 34, 34, 34, 29, 25,  3,  3,  1,\n",
              "       36, 63, 25, 33,  9, 58,  9, 42, 52, 36, 25, 39, 42, 36, 25, 58, 58,\n",
              "       36, 25, 58,  9, 26, 42, 72, 36, 42, 10, 42, 39,  1, 36, 32, 74, 76,\n",
              "       25,  3,  3,  1, 36, 63, 25, 33,  9, 58,  1, 36,  9, 52, 36, 32, 74,\n",
              "       76, 25,  3,  3,  1, 36,  9, 74, 36,  9, 77, 52, 36, 31, 12, 74, 34,\n",
              "       12, 25,  1, 60, 34, 34, 44, 10, 42, 39,  1, 77, 76,  9, 74],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8eBm9UNSHscl"
      },
      "source": [
        "## One-hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NNbhQvcMHscm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5717fe-61f8-4f3e-9bbb-192c20198fbd"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    return np.eye(n_labels,n_labels,  dtype=np.float32)[arr]\n",
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_4ecvZkFHscm"
      },
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QnwHn58kHscn"
      },
      "source": [
        "                M steps ( seq length )\n",
        "               xxx                 xxx\n",
        "               x                     x\n",
        "               x                     x                                Starting sequence:\n",
        "               x                     x                                [1 2 3 4 5 6 7 8 9 10 11 12]\n",
        "               x                     x\n",
        "N batch size   x                     x                                Batch size = 2\n",
        "(No. of steps) x                     x                                [1 2 3 4 5 6]\n",
        "               x                     x                                [7 8 9 10 11 12]\n",
        "               x                     x\n",
        "               x                     x                                Seq length = 3\n",
        "               x                     x\n",
        "               x                     x                                  ┌─────┐\n",
        "               x                     x                                [ │1 2 3│ 4 5 6]\n",
        "               x                     x                                [ │7 8 9│ 10 11 12]\n",
        "               x                     x                                  └─────┘\n",
        "               xxx                 xxx\n",
        "\n",
        "            xxxxxxxxxxxxxx   xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
        "                         xxxxx\n",
        "                          xx\n",
        "\n",
        "                          k= No. of batches = total chars/ N.M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Jq-s8InRHsco"
      },
      "source": [
        "def create_batches(arr, batch_size, seq_length):\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    arr = arr[:n_batches*batch_size_total]\n",
        "    arr = arr.reshape((batch_size,-1))\n",
        "\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KOuURObaHscp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e9f2184-9384-4916-9ec9-aa65d40ef8b7"
      },
      "source": [
        "batches = create_batches(encoded, 8, 50)\n",
        "x, y= next(batches)\n",
        "print('x:\\n', x[:10, :10])\n",
        "print('\\ny:\\n', y[:10, :10])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            " [[41 76 25  3 77 42 39 36 22 34]\n",
            " [52 31 74 36 77 76 25 77 36 25]\n",
            " [42 74 40 36 31 39 36 25 36 63]\n",
            " [52 36 77 76 42 36 45 76  9 42]\n",
            " [36 52 25 12 36 76 42 39 36 77]\n",
            " [45 32 52 52  9 31 74 36 25 74]\n",
            " [36 51 74 74 25 36 76 25 40 36]\n",
            " [59 48 58 31 74 52 26  1 60 36]]\n",
            "\n",
            "y:\n",
            " [[76 25  3 77 42 39 36 22 34 34]\n",
            " [31 74 36 77 76 25 77 36 25 77]\n",
            " [74 40 36 31 39 36 25 36 63 31]\n",
            " [36 77 76 42 36 45 76  9 42 63]\n",
            " [52 25 12 36 76 42 39 36 77 42]\n",
            " [32 52 52  9 31 74 36 25 74 40]\n",
            " [51 74 74 25 36 76 25 40 36 52]\n",
            " [48 58 31 74 52 26  1 60 36 78]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "2uvL80mYHscp"
      },
      "source": [
        "## Defining model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "05xGLCSRHscq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "780d9a20-b267-47f3-81d2-6c853b22aed9"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Pnq5Gc04Hscq"
      },
      "source": [
        "class textgenRNN(nn.Module):\n",
        "    def __init__(self,n_output, n_hidden=256, n_layers=2, drop_prob=0.3, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.lstm = nn.LSTM(n_output, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(drop_prob)\n",
        "        self.fc1 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        out, hid = self.lstm(x, hidden)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        return out, hid\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rLVfosNFHscr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1961266-540c-40d9-efe1-39e67d58b51e"
      },
      "source": [
        "from torchsummary import summary\n",
        "n_hidden=512\n",
        "n_layers=10\n",
        "model = textgenRNN(len(vocab), n_hidden=n_hidden, n_layers=n_layers)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "textgenRNN(\n",
            "  (lstm): LSTM(83, 512, num_layers=10, batch_first=True, dropout=0.3)\n",
            "  (dropout1): Dropout(p=0.3, inplace=False)\n",
            "  (fc1): Linear(in_features=512, out_features=83, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J1EYiUkOHscs"
      },
      "source": [
        "import os\n",
        "if os.path.isdir('checkpoint') and os.path.isfile('./checkpoint/trialckpt.t7'):\n",
        "    model.load_state_dict(torch.load('./checkpoint/trialckpt.t7', map_location=torch.device(device)))"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oL--pDxPHsct"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "t0HSH7QxHsct"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "epochs=20\n",
        "batch_size=128\n",
        "seq_length=100\n",
        "lr=0.030\n",
        "clip=5\n",
        "test_portion=0.1\n",
        "\n",
        "# change model mode to train\n",
        "model.train()\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# create training and validation data\n",
        "data, test_data = train_test_split(encoded, test_size=test_portion, shuffle=False, random_state=0)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "i0ItBh6hHscv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24b06e15-944c-4a1c-aa32-757c67b8ece3"
      },
      "source": [
        "counter = 0\n",
        "n_vocab = len(vocab)\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "\n",
        "    for x, y in create_batches(data, batch_size, seq_length):\n",
        "        counter += 1\n",
        "\n",
        "        # One-hot encode our data and make them Torch tensors\n",
        "        x = one_hot_encode(x, n_vocab)\n",
        "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = model(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optim.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % 100 == 0:\n",
        "            # Get validation loss\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for x, y in create_batches(test_data, batch_size, seq_length):\n",
        "                # One-hot encode our data and make them Torch tensors\n",
        "                x = one_hot_encode(x, n_vocab)\n",
        "                x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                inputs, targets = x, y\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                output, val_h = model(inputs, val_h)\n",
        "                val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            print('==> Saving model ...')\n",
        "\n",
        "            if not os.path.isdir('checkpoint'):\n",
        "                os.mkdir('checkpoint')\n",
        "            torch.save(model.state_dict(), './checkpoint/trialckpt.t7')\n",
        "\n",
        "            model.train() # reset to train mode after iterationg through validation data\n",
        "\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Saving model ...\n",
            "Epoch: 1/20... Step: 100... Loss: 3.1196... Val Loss: 3.1285\n",
            "==> Saving model ...\n",
            "Epoch: 2/20... Step: 200... Loss: 3.1090... Val Loss: 3.1296\n",
            "==> Saving model ...\n",
            "Epoch: 3/20... Step: 300... Loss: 3.1288... Val Loss: 3.1281\n",
            "==> Saving model ...\n",
            "Epoch: 3/20... Step: 400... Loss: 3.1156... Val Loss: 3.1244\n",
            "==> Saving model ...\n",
            "Epoch: 4/20... Step: 500... Loss: 3.1048... Val Loss: 3.1301\n",
            "==> Saving model ...\n",
            "Epoch: 5/20... Step: 600... Loss: 3.0841... Val Loss: 3.1279\n",
            "==> Saving model ...\n",
            "Epoch: 6/20... Step: 700... Loss: 3.1013... Val Loss: 3.1278\n",
            "==> Saving model ...\n",
            "Epoch: 6/20... Step: 800... Loss: 3.1136... Val Loss: 3.1302\n",
            "==> Saving model ...\n",
            "Epoch: 7/20... Step: 900... Loss: 3.0893... Val Loss: 3.1302\n",
            "==> Saving model ...\n",
            "Epoch: 8/20... Step: 1000... Loss: 3.1272... Val Loss: 3.1289\n",
            "==> Saving model ...\n",
            "Epoch: 8/20... Step: 1100... Loss: 3.1170... Val Loss: 3.1281\n",
            "==> Saving model ...\n",
            "Epoch: 9/20... Step: 1200... Loss: 3.0981... Val Loss: 3.1275\n",
            "==> Saving model ...\n",
            "Epoch: 10/20... Step: 1300... Loss: 3.0966... Val Loss: 3.1285\n",
            "==> Saving model ...\n",
            "Epoch: 11/20... Step: 1400... Loss: 3.1210... Val Loss: 3.1290\n",
            "==> Saving model ...\n",
            "Epoch: 11/20... Step: 1500... Loss: 3.1253... Val Loss: 3.1267\n",
            "==> Saving model ...\n",
            "Epoch: 12/20... Step: 1600... Loss: 3.1170... Val Loss: 3.1287\n",
            "==> Saving model ...\n",
            "Epoch: 13/20... Step: 1700... Loss: 3.1192... Val Loss: 3.1302\n",
            "==> Saving model ...\n",
            "Epoch: 13/20... Step: 1800... Loss: 3.1144... Val Loss: 3.1278\n",
            "==> Saving model ...\n",
            "Epoch: 14/20... Step: 1900... Loss: 3.1175... Val Loss: 3.1301\n",
            "==> Saving model ...\n",
            "Epoch: 15/20... Step: 2000... Loss: 3.1023... Val Loss: 3.1293\n",
            "==> Saving model ...\n",
            "Epoch: 16/20... Step: 2100... Loss: 3.1149... Val Loss: 3.1329\n",
            "==> Saving model ...\n",
            "Epoch: 16/20... Step: 2200... Loss: 3.1036... Val Loss: 3.1282\n",
            "==> Saving model ...\n",
            "Epoch: 17/20... Step: 2300... Loss: 3.1071... Val Loss: 3.1275\n",
            "==> Saving model ...\n",
            "Epoch: 18/20... Step: 2400... Loss: 3.1013... Val Loss: 3.1335\n",
            "==> Saving model ...\n",
            "Epoch: 18/20... Step: 2500... Loss: 3.1040... Val Loss: 3.1299\n",
            "==> Saving model ...\n",
            "Epoch: 19/20... Step: 2600... Loss: 3.0948... Val Loss: 3.1288\n",
            "==> Saving model ...\n",
            "Epoch: 20/20... Step: 2700... Loss: 3.1110... Val Loss: 3.1289\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OJWkbwyKHsc5"
      },
      "source": [
        "# Predicting next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0a1qYFEEHsc-"
      },
      "source": [
        "def predict(model, inputs, h=None, top_k= None):\n",
        "    x = np.array([[vocab_to_int[inputs]]])\n",
        "    x = one_hot_encode(x, len(vocab))\n",
        "    inp = torch.from_numpy(x).to(device)\n",
        "\n",
        "    # get access to hidden state\n",
        "    h = tuple([i.data for i in h])\n",
        "    # pass inputs and hidden state to model\n",
        "    out, h = model(inp, h)\n",
        "\n",
        "    # get prob of the char\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    p = p.to('cpu')\n",
        "\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(vocab))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    p = p.numpy().squeeze()\n",
        "    v = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "    return int_to_vocab[v], h\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "O_gxWBx9Hsc_"
      },
      "source": [
        "# Sample text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "exOuMPOxHsc_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ab3bbb2-118f-4771-ccd1-3e9865905f24"
      },
      "source": [
        "def sample(model, size, prime='The ', top_k=None):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval() # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = model.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)\n",
        "print(sample(model, 2000, prime='The ', top_k=5))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The      etea   to   o ta  oooeaet et at o a a o  oe e ttoee oteet   tetoe otota t e  a  t eo eteatoae     oeto  te eae at  ota  t   oe etet oetaea  t    otoooeeat e a tt aoa t ote etataateea   o tt etatt aeo e  ote aa  eaat  ae    eat    oaetaot  a ot eete oea   ta  teeoaetoe tta ae  teae oetote taeaetoe a tee t aoee ea e   ato  a  ttoa  e   e a aae  o  eta      a  aoeo e     a eooo e  eoeao tta e  too e      a a o e oa ao a e t eto otet t a tea  oet aat  at eae t  ee eeoeeoa tt o tett to ota    oot oaete    aoe a e  tttaaoeo te a tae o aao oaee to  e  e eeaa a e  oeotaeo  ee e a e  ot too ttoeeaoe oe eae taotaoetoo oetoe e taoet e o at  a   atee te t oeeoeaooaettta  ao to eteo e to  totae tttea    tt eeeoeo eo et o ae  et  ot  a  a  eea   ae te t  e  a  e  o  e  a te to ota  t ooee   aa a   aa t  ee oe to teao  t  aa   eoto oa te a  eeoaoaet   t oo  o   o  tet ao oaao t attte a  eota a ee  o   ot oaaa t e  eea  ototoe ot oa   ta e  ta a  aet   a o o    eoeae et te oe e a ae otetooo o  t t e  eat aettaa  oaet   t   tt o  eee e eoaoetoe  oo otoeoe ee a   eoo o    taeootao ote  o atetta  t a   to ao tt  e    oo otaa  o  aa ootet t  ettat    toaeeo t e eee    ott t    tatoatttoeot   tt et eeee o  ea  a  o  eoa  ateo  aoo eaa o aea   o otaa a  aa eeotoetoo     eattae  oa  a t  eat to et aea e   eeateat teaa e tao  ae  eaa  e  a  eo   a eao oa t a  e tet eootet eota  t o eooeae  oee  ooo   ea  taoo  eta  etet o  o t   ae   o t o   a ee aooeoeee a   ooe a ooao    tot  te  etea ta  oeata  teoett tt e ee    o eo  ote o  o     t  oat et oot ta ao t  ott  a    ot  tt t  e aae aoteeoettt     e ae t  o   ot  t t ea te  oao  oeoo  eaae eea  te  eooa e otoea     t tao  aa e teot  otttttteo ee    e      o  t   ett o t e o t  ata oaea  aa o aoat  eto ee e e a t t aet oo a oeoe  a  o    ateot  a ee tet e etatee e toa e aoo e  o ee e  aeaee   toeo ta  oo  teteoootte   eott   o e     t o  tt e t te o   oee   eteo  e  ao  eo   teo ete  e ae t oe  a aee aoe  e e  e       e  eoa  t     eta  \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}