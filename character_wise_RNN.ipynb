{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "character_wise_RNN.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "sby3EvKFHscQ"
      },
      "source": [
        "# character-wise RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "DDzwBY87Hscd"
      },
      "source": [
        "![Overview](https://github.com/udacity/deep-learning/raw/78c91a5607ecfdc29b762e45c082d7ca5047c8a1/intro-to-rnns/assets/charseq.jpeg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "JG4ymNGZHsce"
      },
      "source": [
        "\n",
        "import time\n",
        "from collections import namedtuple\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(0)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "u7IQJtsLHscg"
      },
      "source": [
        "## Loading data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "P3SKxnVwHsch",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "30227c70-2693-48d6-b4ab-8c3d4fb4f533"
      },
      "source": [
        "!curl http://www.gutenberg.org/files/2591/2591-0.txt --create-dirs -o .pytorch/trialReport/trial.txt\n",
        "with open('.pytorch/trialReport/trial.txt') as r:\n",
        "    reports = r.read()\n",
        "reports[:100]"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  547k  100  547k    0     0   931k      0 --:--:-- --:--:-- --:--:--  931k\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ufeffThe Project Gutenberg EBook of Grimms’ Fairy Tales, by The Brothers Grimm\\n\\nThis eBook is for the us'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4evrtXuFkz-",
        "outputId": "287eec50-d2fa-4499-df82-369218808d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "words = re.split(r'\\W+', reports)\n",
        "words = [word.lower() for word in words]\n",
        "print(words[:100])"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['', 'the', 'project', 'gutenberg', 'ebook', 'of', 'grimms', 'fairy', 'tales', 'by', 'the', 'brothers', 'grimm', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 're', 'use', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'www', 'gutenberg', 'org', 'title', 'grimms', 'fairy', 'tales', 'author', 'the', 'brothers', 'grimm', 'translator', 'edgar', 'taylor', 'and', 'marian', 'edwardes', 'posting', 'date', 'december', '14', '2008', 'ebook', '2591', 'release', 'date', 'april', '2001', 'last', 'updated', 'november', '7', '2016', 'language', 'english', 'character', 'set', 'encoding', 'utf', '8', 'start', 'of', 'this']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBvMLLuvFko2"
      },
      "source": [
        "reports = ' '.join(words)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "rm5g3PVPHscj"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "eK0cXQ7DHsck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbcd55a9-fcf7-4f91-b414-6b37f77ef037"
      },
      "source": [
        "vocab = tuple(set(reports))\n",
        "print('vocabs:\\n', vocab)\n",
        "int_to_vocab = dict(enumerate(vocab))\n",
        "print('int to vocab:\\n', int_to_vocab)\n",
        "vocab_to_int = {v: i for i,v in int_to_vocab.items()}\n",
        "encoded = np.array([vocab_to_int[v] for v in reports], dtype=np.int32)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabs:\n",
            " ('b', 'p', '6', '9', 'u', 'g', ' ', 's', 'n', 'y', 'r', '2', '3', 'i', 'f', 'a', 'v', 'h', 'z', 'k', 'x', 'q', 'o', '5', '4', 't', 'w', 'd', '8', '_', 'c', 'l', '7', 'e', 'j', '0', '1', 'm')\n",
            "int to vocab:\n",
            " {0: 'b', 1: 'p', 2: '6', 3: '9', 4: 'u', 5: 'g', 6: ' ', 7: 's', 8: 'n', 9: 'y', 10: 'r', 11: '2', 12: '3', 13: 'i', 14: 'f', 15: 'a', 16: 'v', 17: 'h', 18: 'z', 19: 'k', 20: 'x', 21: 'q', 22: 'o', 23: '5', 24: '4', 25: 't', 26: 'w', 27: 'd', 28: '8', 29: '_', 30: 'c', 31: 'l', 32: '7', 33: 'e', 34: 'j', 35: '0', 36: '1', 37: 'm'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "iN-rQg0pHsck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c25972c-802d-4ede-d546-4a2312f02e64"
      },
      "source": [
        "encoded[:100]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 6, 25, 17, 33,  6,  1, 10, 22, 34, 33, 30, 25,  6,  5,  4, 25, 33,\n",
              "        8,  0, 33, 10,  5,  6, 33,  0, 22, 22, 19,  6, 22, 14,  6,  5, 10,\n",
              "       13, 37, 37,  7,  6, 14, 15, 13, 10,  9,  6, 25, 15, 31, 33,  7,  6,\n",
              "        0,  9,  6, 25, 17, 33,  6,  0, 10, 22, 25, 17, 33, 10,  7,  6,  5,\n",
              "       10, 13, 37, 37,  6, 25, 17, 13,  7,  6, 33,  0, 22, 22, 19,  6, 13,\n",
              "        7,  6, 14, 22, 10,  6, 25, 17, 33,  6,  4,  7, 33,  6, 22],\n",
              "      dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "8eBm9UNSHscl"
      },
      "source": [
        "## One-hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "NNbhQvcMHscm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e00af84-748e-4467-e46a-efbf6efc5a28"
      },
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "\n",
        "    return np.eye(n_labels,n_labels,  dtype=np.float32)[arr]\n",
        "# check that the function works as expected\n",
        "test_seq = np.array([[3, 5, 1]])\n",
        "one_hot = one_hot_encode(test_seq, 8)\n",
        "\n",
        "print(one_hot)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "_4ecvZkFHscm"
      },
      "source": [
        "## Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QnwHn58kHscn"
      },
      "source": [
        "                M steps ( seq length )\n",
        "               xxx                 xxx\n",
        "               x                     x\n",
        "               x                     x                                Starting sequence:\n",
        "               x                     x                                [1 2 3 4 5 6 7 8 9 10 11 12]\n",
        "               x                     x\n",
        "N batch size   x                     x                                Batch size = 2\n",
        "(No. of steps) x                     x                                [1 2 3 4 5 6]\n",
        "               x                     x                                [7 8 9 10 11 12]\n",
        "               x                     x\n",
        "               x                     x                                Seq length = 3\n",
        "               x                     x\n",
        "               x                     x                                  ┌─────┐\n",
        "               x                     x                                [ │1 2 3│ 4 5 6]\n",
        "               x                     x                                [ │7 8 9│ 10 11 12]\n",
        "               x                     x                                  └─────┘\n",
        "               xxx                 xxx\n",
        "\n",
        "            xxxxxxxxxxxxxx   xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
        "                         xxxxx\n",
        "                          xx\n",
        "\n",
        "                          k= No. of batches = total chars/ N.M"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Jq-s8InRHsco"
      },
      "source": [
        "def create_batches(arr, batch_size, seq_length):\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batches = len(arr) // batch_size_total\n",
        "    arr = arr[:n_batches*batch_size_total]\n",
        "    arr = arr.reshape((batch_size,-1))\n",
        "\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        yield x, y"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KOuURObaHscp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb4813d8-caa7-40b0-bd7a-159083843569"
      },
      "source": [
        "batches = create_batches(encoded, 8, 50)\n",
        "x, y= next(batches)\n",
        "print('x:\\n', x[:10, :10])\n",
        "print('\\ny:\\n', y[:10, :10])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            " [[ 6 25 17 33  6  1 10 22 34 33]\n",
            " [ 7  6 15  8 27  6 25 17 33  6]\n",
            " [ 0  4 10 27 33  8  6  1 15 10]\n",
            " [ 7 19  6 25 17 33  6 10 13  8]\n",
            " [17  6  9 22  4  6 26 22  4 31]\n",
            " [ 7 25 10 15  8  5 33  6 25 22]\n",
            " [22  8  6 25 17 33 13 10  6 14]\n",
            " [10 27  6 15  6  5 10 33 15 25]]\n",
            "\n",
            "y:\n",
            " [[25 17 33  6  1 10 22 34 33 30]\n",
            " [ 6 15  8 27  6 25 17 33  6  7]\n",
            " [ 4 10 27 33  8  6  1 15 10 25]\n",
            " [19  6 25 17 33  6 10 13  8  5]\n",
            " [ 6  9 22  4  6 26 22  4 31 27]\n",
            " [25 10 15  8  5 33  6 25 22 26]\n",
            " [ 8  6 25 17 33 13 10  6 14 15]\n",
            " [27  6 15  6  5 10 33 15 25  6]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "2uvL80mYHscp"
      },
      "source": [
        "## Defining model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "05xGLCSRHscq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c7acc76-425b-4ac7-b12a-613f899e8926"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Pnq5Gc04Hscq"
      },
      "source": [
        "class textgenRNN(nn.Module):\n",
        "    def __init__(self,n_output, n_hidden=256, n_layers=2, drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        self.n_output = n_output\n",
        "\n",
        "        self.lstm = nn.LSTM(n_output, n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
        "        self.dropout1 = nn.Dropout(drop_prob)\n",
        "        self.fc1 = nn.Linear(n_hidden, n_output)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        out, hid = self.lstm(x, hidden)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "        return out, hid\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
        "\n",
        "        return hidden"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "rLVfosNFHscr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4f72be-e43d-4f95-d641-ffcc31c49c87"
      },
      "source": [
        "from torchsummary import summary\n",
        "n_hidden=512\n",
        "n_layers=2\n",
        "model = textgenRNN(len(vocab), n_hidden=n_hidden, n_layers=n_layers)\n",
        "model.to(device)\n",
        "print(model)"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "textgenRNN(\n",
            "  (lstm): LSTM(38, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout1): Dropout(p=0.5, inplace=False)\n",
            "  (fc1): Linear(in_features=512, out_features=38, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "J1EYiUkOHscs"
      },
      "source": [
        "import os\n",
        "if os.path.isdir('checkpoint') and os.path.isfile('./checkpoint/trialckpt.t7'):\n",
        "    model.load_state_dict(torch.load('./checkpoint/trialckpt.t7', map_location=torch.device(device)))"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "oL--pDxPHsct"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "t0HSH7QxHsct"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "epochs=100\n",
        "batch_size=256\n",
        "seq_length=100\n",
        "lr=0.01\n",
        "clip=5\n",
        "test_portion=0.1\n",
        "\n",
        "# change model mode to train\n",
        "model.train()\n",
        "\n",
        "optim = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# create training and validation data\n",
        "data, test_data = train_test_split(encoded, test_size=test_portion, shuffle=False, random_state=0)"
      ],
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "i0ItBh6hHscv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "a2a92903-f8ee-4685-dd50-c156489d506a"
      },
      "source": [
        "counter = 0\n",
        "n_vocab = len(vocab)\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = model.init_hidden(batch_size)\n",
        "\n",
        "    for x, y in create_batches(data, batch_size, seq_length):\n",
        "        counter += 1\n",
        "\n",
        "        # One-hot encode our data and make them Torch tensors\n",
        "        x = one_hot_encode(x, n_vocab)\n",
        "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        model.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = model(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optim.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % 100 == 0:\n",
        "            # Get validation loss\n",
        "            val_h = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for x, y in create_batches(test_data, batch_size, seq_length):\n",
        "                # One-hot encode our data and make them Torch tensors\n",
        "                x = one_hot_encode(x, n_vocab)\n",
        "                x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                inputs, targets = x, y\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "                output, val_h = model(inputs, val_h)\n",
        "                val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            print('==> Saving model ...')\n",
        "\n",
        "            if not os.path.isdir('checkpoint'):\n",
        "                os.mkdir('checkpoint')\n",
        "            torch.save(model.state_dict(), './checkpoint/trialckpt.t7')\n",
        "\n",
        "            model.train() # reset to train mode after iterationg through validation data\n",
        "\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Saving model ...\n",
            "Epoch: 6/100... Step: 100... Loss: 2.7871... Val Loss: 2.8852\n",
            "==> Saving model ...\n",
            "Epoch: 12/100... Step: 200... Loss: 2.7911... Val Loss: 2.8879\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-b626db5a0190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "OJWkbwyKHsc5"
      },
      "source": [
        "# Predicting next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0a1qYFEEHsc-"
      },
      "source": [
        "def predict(model, inputs, h=None, top_k= None):\n",
        "    x = np.array([[vocab_to_int[inputs]]])\n",
        "    x = one_hot_encode(x, len(vocab))\n",
        "    inp = torch.from_numpy(x).to(device)\n",
        "\n",
        "    # get access to hidden state\n",
        "    h = tuple([i.data for i in h])\n",
        "    # pass inputs and hidden state to model\n",
        "    out, h = model(inp, h)\n",
        "\n",
        "    # get prob of the char\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    p = p.to('cpu')\n",
        "\n",
        "    if top_k is None:\n",
        "        top_ch = np.arange(len(vocab))\n",
        "    else:\n",
        "        p, top_ch = p.topk(top_k)\n",
        "        top_ch = top_ch.numpy().squeeze()\n",
        "\n",
        "    p = p.numpy().squeeze()\n",
        "    v = np.random.choice(top_ch, p=p/p.sum())\n",
        "\n",
        "    return int_to_vocab[v], h\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "O_gxWBx9Hsc_"
      },
      "source": [
        "# Sample text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "exOuMPOxHsc_"
      },
      "source": [
        "def sample(model, size, prime='the ', top_k=None):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    model.eval() # eval mode\n",
        "\n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = model.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(model, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "\n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(model, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)\n",
        "print(sample(model, 2000, prime='the ', top_k=26))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}